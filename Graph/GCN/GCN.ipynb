{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "import pickle \n",
    "import numpy as np\n",
    "from math import sqrt\n",
    "import torch.nn.functional as F\n",
    "from timeit import default_timer as timer \n",
    "from tensorboardX import SummaryWriter\n",
    "import os\n",
    "\n",
    "class GCNLayer(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    GCN Hidden Layer\n",
    "    \"\"\"\n",
    "    def __init__(self,input_dim,output_dim,activation=True):\n",
    "        super(GCNLayer,self).__init__()\n",
    "        self.linear=torch.nn.Linear(input_dim,output_dim)\n",
    "        glort_beng=sqrt(6)/sqrt(input_dim+output_dim)\n",
    "        self.linear.weight.data.uniform_(-glort_beng,glort_beng).float()\n",
    "        self.activation=torch.nn.ReLU() if activation else None\n",
    "            \n",
    "    def forward(self,input_data):\n",
    "        \"\"\"\n",
    "        input : First Layer - Node X 16, Second Layer - 16 X # Label\n",
    "        \"\"\"\n",
    "        output=self.linear(input_data)\n",
    "        return self.activation(output) if self.activation!=None else output\n",
    "\n",
    "    \n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super(GCN,self).__init__()\n",
    "        self.epoch=config['epoch']\n",
    "        self.gcn_l1=GCNLayer(config['input_l1_dim'],config['output_l1_dim'])\n",
    "        self.gcn_l2=GCNLayer(config['output_l1_dim'],config['output_l2_dim'],activation=False)\n",
    "        self.dropout=torch.nn.Dropout(config['dropout'])\n",
    "        self.lr=config['lr']\n",
    "        self.best_accuracy=0.0\n",
    "        self.max_trial=5\n",
    "        self.cur_trial=0\n",
    "        log_dir=os.path.join('cora')\n",
    "        if not os.path.exists(log_dir):\n",
    "            os.makedirs(log_dir)\n",
    "        self.writer=SummaryWriter(log_dir)\n",
    "        \n",
    "    def preprocess_A_hat(self,adj_mtx):\n",
    "        \"\"\"\n",
    "        return - A Hat\n",
    "        Input - Adjacency Matrix(Node X Node)\n",
    "        \"\"\"\n",
    "        # A_hat\n",
    "        I=np.eye(adj_mtx.shape[0])\n",
    "        # Add Self loop\n",
    "        A_=adj_mtx+I\n",
    "        D_=np.sum(A_,axis=1)\n",
    "        D_inv_sqrt=np.power(D_,-0.5)\n",
    "        D_inv_sqrt[np.isinf(D_inv_sqrt)]=0.0\n",
    "        D_inv_sqrt=np.diag(D_inv_sqrt)\n",
    "        A_hat=np.dot(np.dot(D_inv_sqrt,A_),D_inv_sqrt)\n",
    "        return torch.from_numpy(A_hat).cuda()\n",
    "        \n",
    "    def forward(self, A_hat, feature):\n",
    "        \"\"\"\n",
    "        parmeter : Preprocessed Adjacency matrix, Feature array (Node(V) X # of Feature)\n",
    "        \"\"\"\n",
    "        arr_layer_1=torch.mm(A_hat,feature).float()\n",
    "        arr_layer_1=self.gcn_l1(arr_layer_1)\n",
    "        arr_layer_1=self.dropout(arr_layer_1)\n",
    "        arr_layer_2=torch.mm(A_hat.float(),arr_layer_1)\n",
    "        arr_layer_2=self.gcn_l2(arr_layer_2)\n",
    "        return F.log_softmax(arr_layer_2,dim=1)\n",
    "    \n",
    "    def accuracy(self,output,label,msk):\n",
    "        # predict\n",
    "        predict_class=torch.argmax(output,dim=1)\n",
    "        # target\n",
    "        ratio_correct= (predict_class == label)\n",
    "        correct=torch.sum(ratio_correct.float()*msk)\n",
    "        return correct/torch.sum(msk)\n",
    "    \n",
    "    @staticmethod    \n",
    "    def run(model,train_msk,valid_msk,test_msk,label,adj,feature_cat):\n",
    "        label=torch.argmax(label,dim=1)\n",
    "        A_hat=model.preprocess_A_hat(adj)\n",
    "        start_total=timer()\n",
    "        optimizer=torch.optim.Adam(model.parameters(),lr=model.lr)\n",
    "        for e in range(model.epoch):\n",
    "            start=timer()\n",
    "            model.train()\n",
    "            output=model(A_hat,feature_cat)\n",
    "            train_loss=F.nll_loss(output[train_msk],label[train_msk])\n",
    "            model.eval()\n",
    "            valid_accuracy=model.accuracy(output,label,valid_msk).item()\n",
    "#             if not model.early_stop(valid_accuracy):\n",
    "#                 print(\"Last Epoch : {:d}, accuracy : {:.4f}\".format(e+1,valid_accuracy))\n",
    "#                 break\n",
    "           \n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "            model.writer.add_scalars('loss',{'train_loss':train_loss.item()},e)\n",
    "            model.writer.add_scalars('accuracy',{'valid_loss':valid_accuracy},e)\n",
    "            if e%20 ==0:\n",
    "                print(\"Epoch : {:d}, Train Loss : {:.4f}, Accuracy : {:.4f}, Time : {:.4f}\".format(e+1,train_loss.item(),valid_accuracy,timer()-start))\n",
    "        model.eval()\n",
    "        output=model(A_hat,feature_cat)\n",
    "        test_accuracy=F.nll_loss(output[test_msk],label[test_msk]).item()\n",
    "        print(\"Accuracy : {:.4f}, Time : {:.4f}\".format(test_accuracy,timer()-start_total))\n",
    "        model.writer.close()\n",
    "            \n",
    "            \n",
    "    def early_stop(self,accur):\n",
    "        \"\"\"\n",
    "        For Early Stop\n",
    "        \"\"\"\n",
    "        if self.best_accuracy<accur:\n",
    "            self.cur_trial=0\n",
    "            self.best_accuracy=accur\n",
    "            return True\n",
    "        elif self.max_trial>self.cur_trial:\n",
    "            self.cur_trial+=1\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj, feature_cat,train_label,test_label,valid_label, train_msk, test_msk, valid_msk, label=load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "config={\n",
    "    'epoch':200,\n",
    "    'input_l1_dim':feature_cat.shape[1],\n",
    "    'output_l1_dim':16,\n",
    "    'output_l2_dim':train_label.shape[1],\n",
    "    'dropout':0.5,\n",
    "    'lr':0.01\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1, Train Loss : 1.9516, Accuracy : 0.0760, Time : 0.0610\n",
      "Epoch : 21, Train Loss : 1.6790, Accuracy : 0.4980, Time : 0.0495\n",
      "Epoch : 41, Train Loss : 1.2519, Accuracy : 0.5960, Time : 0.0496\n",
      "Epoch : 61, Train Loss : 0.7694, Accuracy : 0.6460, Time : 0.0497\n",
      "Epoch : 81, Train Loss : 0.4886, Accuracy : 0.7140, Time : 0.0495\n",
      "Epoch : 101, Train Loss : 0.2991, Accuracy : 0.6580, Time : 0.0497\n",
      "Epoch : 121, Train Loss : 0.2030, Accuracy : 0.6920, Time : 0.0498\n",
      "Epoch : 141, Train Loss : 0.1636, Accuracy : 0.7180, Time : 0.0499\n",
      "Epoch : 161, Train Loss : 0.1625, Accuracy : 0.6980, Time : 0.0501\n",
      "Epoch : 181, Train Loss : 0.1142, Accuracy : 0.6800, Time : 0.0501\n",
      "Accuracy : 0.7542, Time : 9.9978\n"
     ]
    }
   ],
   "source": [
    "gcn=GCN(config).cuda()\n",
    "GCN.run(gcn,train_msk,valid_msk,test_msk,label,adj,feature_cat)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
