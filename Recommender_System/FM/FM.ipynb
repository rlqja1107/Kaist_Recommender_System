{ 
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from timeit import default_timer as timer\n",
    "import numpy as np\n",
    "from math import sqrt\n",
    "\n",
    "class FactorizationMachine(torch.nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super(FactorizationMachine,self).__init__()\n",
    "        self.n_feature=0\n",
    "        self.weight_decay=config['weight_decay']\n",
    "        self.n_embedding=config['n_embedding']\n",
    "        self.lr=config['lr']\n",
    "        self.batch_size=config['batch_size']\n",
    "        self.epoch=config['epoch']\n",
    "        self.is_classifier=config['is_classifier']\n",
    "        self.loss_func=torch.nn.BCELoss() if self.is_classifier else torch.nn.MSELoss(reduction='sum')\n",
    "        # for early stop\n",
    "        self.best_auc=0\n",
    "        self.best_RMSE=100\n",
    "        self.back_times=0\n",
    "        self.limit_trial=3\n",
    "#         self.dataset=MovieLensDataSet()\n",
    "#         self.set_embedding(self.dataset)\n",
    "        \n",
    "    def forward(self,user_item):\n",
    "        \"\"\"\n",
    "        user_item : batch_size X 2 (user, item)\n",
    "        \"\"\"\n",
    "        # Linear sum\n",
    "        user_item=user_item+self.offset\n",
    "        linear_total=torch.sum(self.linear_emb(user_item),dim=1)+self.w_0\n",
    "        # Quad sum\n",
    "        x=self.quad_emb(user_item)\n",
    "        square_of_sum=torch.sum(x,dim=1)**2\n",
    "        sum_of_square=torch.sum(x**2,dim=1)\n",
    "        cal=square_of_sum-sum_of_square\n",
    "        cal=torch.sum(cal,dim=1,keepdim=True)\n",
    "        \n",
    "        quad_total=0.5*cal\n",
    "        # Total sum  \n",
    "        return torch.sigmoid((linear_total+quad_total).squeeze(1)) if self.is_classifier else (linear_total+quad_total).squeeze(1)\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def train_dataset(model,train_loader,optim):\n",
    "        \"\"\"\n",
    "        train_loader : batch X 2, optim : Adaptive Gradient Moment\n",
    "        \"\"\"\n",
    "        model.train()\n",
    "        total_loss=0\n",
    "        for index,batch in enumerate(train_loader):\n",
    "            user_item=batch[0]\n",
    "            target=batch[1]\n",
    "            #forward \n",
    "            y=model(user_item)\n",
    "            loss=model.loss_func(y,target.float())\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            total_loss+=loss.item()\n",
    "            \n",
    "    @staticmethod \n",
    "    def test(model,valid_loader):\n",
    "        model.eval()\n",
    "        target_list=list()\n",
    "        predict_list=list()\n",
    "        with torch.no_grad():\n",
    "            for index,batch in enumerate(valid_loader):\n",
    "                user_item=batch[0]\n",
    "                target=batch[1]\n",
    "                y=model(user_item)\n",
    "                target_list.extend(target.tolist())\n",
    "                predict_list.extend(y.tolist())\n",
    "        \n",
    "        return roc_auc_score(target_list,predict_list) if model.is_classifier else sqrt(model.loss_func(torch.tensor(target_list),torch.tensor(predict_list)).item()/len(target_list))\n",
    "            \n",
    "        \n",
    "    def set_embedding(self,dataset):\n",
    "        feature_dim=dataset.feature_dim.cpu().numpy()\n",
    "        # w_i\n",
    "        self.linear_emb=torch.nn.Embedding(torch.tensor(sum(feature_dim)),1).cuda()\n",
    "        # v_i, v_j\n",
    "        self.quad_emb=torch.nn.Embedding(torch.tensor(sum(feature_dim)),self.n_embedding).cuda()\n",
    "        # initialize the weight in embedding\n",
    "        torch.nn.init.xavier_uniform_(self.quad_emb.weight.data)\n",
    "        self.w_0=torch.nn.Parameter(torch.rand(1,dtype=torch.float),requires_grad=True).cuda()\n",
    "        self.offset=torch.tensor(np.array((0,feature_dim[0]))).cuda()\n",
    "#         dataset.user_item=torch.tensor(dataset.user_item).cuda()\n",
    "#         dataset.feature_dim=torch.tensor(dataset.feature_dim).cuda()      \n",
    "#         dataset.target=torch.tensor(dataset.target).cuda()  \n",
    "        \n",
    "    @staticmethod\n",
    "    def run(model,config,dataset):\n",
    "  \n",
    "        model.set_embedding(dataset)\n",
    "        train_data, valid_data, test_data = torch.utils.data.random_split(dataset,(dataset.train_length,dataset.valid_length,dataset.test_length))\n",
    "        train_loader=DataLoader(train_data,batch_size=model.batch_size)\n",
    "        test_loader=DataLoader(test_data,batch_size=model.batch_size)\n",
    "        valid_loader=DataLoader(valid_data,batch_size=model.batch_size)\n",
    "        optim=torch.optim.Adam(model.parameters(),lr=model.lr)\n",
    "        for e in range(model.epoch):\n",
    "            start=timer()\n",
    "            FactorizationMachine.train_dataset(model,train_loader,optim)\n",
    "            score=FactorizationMachine.test(model,valid_loader)\n",
    "            if not model.is_continous(score):\n",
    "                print(\"Epoch : {:d} Early Stop! Best AUC : {:.4f}\".format(e,score))\n",
    "                break\n",
    "            print(\"Epoch : {:d} AUC or RMSE : {:.4f} Time : {:.4f}\".format(e+1,score,timer()-start))\n",
    "        score=FactorizationMachine.test(model,test_loader)\n",
    "        print(\"Best AUC or RMSE : {:.4f}\".format(score))\n",
    "    \n",
    "        \n",
    "        \n",
    "    def is_continous(self,cur_score):\n",
    "      \n",
    "        \"\"\"\n",
    "        cur_auc : current auc\n",
    "        if auc value is reversely smaller, reduce the trial.\n",
    "        For early stop, return False\n",
    "        is_classifier : False - Regression, True - Classifier\n",
    "        \"\"\"\n",
    "        # Regression\n",
    "        if not self.is_classifier:\n",
    "            if cur_score<self.best_RMSE:\n",
    "                self.best_RMSE=cur_score\n",
    "                self.back_times=0\n",
    "                return True\n",
    "            elif self.limit_trial>self.back_times:\n",
    "                self.back_times+=1\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "        # Classifier        \n",
    "        if cur_score>self.best_auc:\n",
    "            self.best_auc=cur_score\n",
    "            self.back_times=0\n",
    "            return True\n",
    "        elif self.limit_trial>self.back_times:\n",
    "            self.back_times+=1\n",
    "            \n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=MovieLensDataSet(torch.tensor(rating_data).cuda())\n",
    "fm=FactorizationMachine(config).cuda()\n",
    "FactorizationMachine.run(fm,config,dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=torch.tensor([1,2,3],dtype=torch.float)\n",
    "b=torch.tensor([3,6,8],dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.nn.MSELoss(a,b,reduction='sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda is Available, Use Cuda 0\n",
      "Epoch : 1 AUC or RMSE : 1.7322 Time : 8.8438\n",
      "Epoch : 2 AUC or RMSE : 1.1359 Time : 7.9242\n",
      "Epoch : 3 AUC or RMSE : 1.0191 Time : 7.9408\n",
      "Epoch : 4 AUC or RMSE : 0.9799 Time : 7.9575\n",
      "Epoch : 5 AUC or RMSE : 0.9623 Time : 7.9110\n",
      "Epoch : 6 AUC or RMSE : 0.9524 Time : 7.9343\n",
      "Epoch : 7 AUC or RMSE : 0.9455 Time : 8.1445\n",
      "Epoch : 8 AUC or RMSE : 0.9399 Time : 8.8234\n",
      "Epoch : 9 AUC or RMSE : 0.9348 Time : 8.8534\n",
      "Epoch : 10 AUC or RMSE : 0.9299 Time : 7.9045\n",
      "Epoch : 11 AUC or RMSE : 0.9249 Time : 7.9528\n",
      "Epoch : 12 AUC or RMSE : 0.9199 Time : 7.8247\n",
      "Epoch : 13 AUC or RMSE : 0.9150 Time : 7.8889\n",
      "Epoch : 14 AUC or RMSE : 0.9107 Time : 7.9714\n",
      "Epoch : 15 AUC or RMSE : 0.9069 Time : 7.9241\n",
      "Epoch : 16 AUC or RMSE : 0.9037 Time : 7.9579\n",
      "Epoch : 17 AUC or RMSE : 0.9007 Time : 7.9051\n",
      "Epoch : 18 AUC or RMSE : 0.8980 Time : 7.9467\n",
      "Epoch : 19 AUC or RMSE : 0.8953 Time : 7.9407\n",
      "Epoch : 20 AUC or RMSE : 0.8927 Time : 7.8953\n",
      "Epoch : 21 AUC or RMSE : 0.8901 Time : 7.9345\n",
      "Epoch : 22 AUC or RMSE : 0.8876 Time : 7.8419\n",
      "Epoch : 23 AUC or RMSE : 0.8852 Time : 7.9143\n",
      "Epoch : 24 AUC or RMSE : 0.8829 Time : 8.0098\n",
      "Epoch : 25 AUC or RMSE : 0.8808 Time : 7.9463\n",
      "Epoch : 26 AUC or RMSE : 0.8788 Time : 8.0485\n",
      "Epoch : 27 AUC or RMSE : 0.8771 Time : 7.8915\n",
      "Epoch : 28 AUC or RMSE : 0.8756 Time : 7.9498\n",
      "Epoch : 29 AUC or RMSE : 0.8744 Time : 7.9572\n",
      "Epoch : 30 AUC or RMSE : 0.8734 Time : 7.9162\n",
      "Epoch : 31 AUC or RMSE : 0.8728 Time : 7.9445\n",
      "Epoch : 32 AUC or RMSE : 0.8725 Time : 7.8441\n",
      "Epoch : 33 AUC or RMSE : 0.8726 Time : 7.8858\n",
      "Epoch : 34 AUC or RMSE : 0.8730 Time : 8.0091\n",
      "Epoch : 35 AUC or RMSE : 0.8736 Time : 7.9790\n",
      "Epoch : 36 AUC or RMSE : 0.8745 Time : 7.9577\n",
      "Epoch : 37 AUC or RMSE : 0.8755 Time : 7.8848\n",
      "Epoch : 38 AUC or RMSE : 0.8768 Time : 7.9401\n",
      "Epoch : 39 AUC or RMSE : 0.8781 Time : 7.9457\n",
      "Epoch : 40 AUC or RMSE : 0.8796 Time : 7.8999\n",
      "Epoch : 41 AUC or RMSE : 0.8811 Time : 7.9665\n",
      "Epoch : 42 AUC or RMSE : 0.8826 Time : 7.8495\n",
      "Epoch : 43 AUC or RMSE : 0.8841 Time : 7.9151\n",
      "Epoch : 44 AUC or RMSE : 0.8856 Time : 7.9984\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-89e2a39b1c91>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMovieLensDataSet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrating_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'is_classifier'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mfm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFactorizationMachine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mFactorizationMachine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-10-33c592515c9a>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(model, config, dataset)\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m             \u001b[0mFactorizationMachine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m             \u001b[0mscore\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFactorizationMachine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalid_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_continous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-33c592515c9a>\u001b[0m in \u001b[0;36mtrain_dataset\u001b[0;34m(model, train_loader, optim)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m             \u001b[0mtotal_loss\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/kibum/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/kibum/lib/python3.7/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    117\u001b[0m                    \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m                    \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'weight_decay'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m                    \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m                    )\n\u001b[1;32m    121\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/kibum/lib/python3.7/site-packages/torch/optim/functional.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0;31m# Maintains the maximum of all 2nd moment running avg. till now\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=torch.tensor([[2,3,4],[3,4,5],[4,8,9]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
